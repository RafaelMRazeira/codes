{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# sp_data is a pandas dataframe of our data\n",
    "sp_data = pd.read_json(\n",
    "    '/media/rafael/D/2020 RAFAEL/Faculdade/ICs/IC Direito/data/sp_shinx.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# COUNT THE SEPARATOR ' - ' IN CONTEUDO OF DATASET\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "count_separator = [] # this is a vector which contains the frequency of ' - ' in each Document\n",
    "\n",
    "count_Counteudo_length = [] # this is a vector which contains the lenght of \"Conteúdo\" in each Document\n",
    "\n",
    "# for each Document in data, count the frequency of ' - ' and the len of \"Conteúdo\"\n",
    "for i, Document in enumerate(sp_data['Conteúdo']):\n",
    "    count_separator.append(Document.count(' - '))\n",
    "    count_Counteudo_length.append(len(Document))\n",
    "\n",
    "data = np.array(count_separator) # vector which contains the amount of ' - ' in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.local/lib/python3.6/site-packages/pandas/core/strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87499"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# FILTER THE CONTEUDO OF NOT SENTENCES DOCUMENTS WHICH HAS 4 OR MORE ' - '\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# regex for search of a subcontent of sentences in our data\n",
    "regex = r\"( julgo | homologo.*o acordo)\"\n",
    "\n",
    "Doc_series = pd.Series(sp_data['Conteúdo']) # Transform the pandas Data.frame to a series for initial interting spot\n",
    "\n",
    "# Filter the documents which has the regex in \"conteúdo\"\n",
    "sentencas = Doc_series.str.contains(regex, case=False)\n",
    "sentencas = sentencas.to_numpy()\n",
    "\n",
    "# ind_range = (data > 5) & (data < 10)\n",
    "ind_range = (data > 4) & (sentencas == False)\n",
    "\n",
    "data_in_6_and_9 = sp_data['Conteúdo'][ind_range]\n",
    "\n",
    "positions = []\n",
    "\n",
    "processos = []\n",
    "\n",
    "classes = []\n",
    "\n",
    "assuntos = []\n",
    "\n",
    "partes = []\n",
    "\n",
    "conteudo = []\n",
    "\n",
    "advs = []\n",
    "\n",
    "for document in data_in_6_and_9:\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the processes\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    position = re.search(r' - ', document)\n",
    "    processos.append(document[0:position.start()])\n",
    "\n",
    "    doc_no_processos = document[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the classes\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r' - ', doc_no_processos)\n",
    "    classes.append(doc_no_processos[0:position.start()])\n",
    "\n",
    "    doc_no_classes_to = doc_no_processos[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the assuntos\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r' - ', doc_no_classes_to)\n",
    "    assuntos.append(doc_no_classes_to[0:position.start()])\n",
    "\n",
    "    doc_no_assuntos_to = doc_no_classes_to[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the parts\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r' - ', doc_no_assuntos_to)\n",
    "    partes.append(doc_no_assuntos_to[0:position.start()])\n",
    "\n",
    "    doc_no_parts_to = doc_no_assuntos_to[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the content\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the ADV\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r\"(?i)- ADV\", doc_no_parts_to)\n",
    "    if position:\n",
    "        advs.append(doc_no_parts_to[position.start():])\n",
    "\n",
    "        conteudo.append(doc_no_parts_to[0: position.start()])\n",
    "    else:\n",
    "        conteudo.append(doc_no_parts_to)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "len(data_in_6_and_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11468, 11480)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# FILTER THE CONTEUDO OF SENTENCES DOCUMENTS WHICH HAS 4 OR MORE ' - '\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "C_sentencas = sp_data[sentencas]['Conteúdo']\n",
    "cont_sentencas = data[sentencas]\n",
    "\n",
    "more = cont_sentencas > 4\n",
    "\n",
    "teste = C_sentencas[more]\n",
    "\n",
    "processos_sentencas = []\n",
    "\n",
    "classes_sentencas = []\n",
    "\n",
    "assuntos_sentencas = []\n",
    "\n",
    "partes_sentencas = []\n",
    "\n",
    "conteudo_sentencas = []\n",
    "\n",
    "advs_sentencas = []\n",
    "\n",
    "for i, document in enumerate(teste):\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the processes\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    position = re.search(r' - ', document)\n",
    "    if position:\n",
    "        processos_sentencas.insert(i, document[0:position.start()])\n",
    "\n",
    "        doc_no_processos = document[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the classes\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r' - ', doc_no_processos)\n",
    "\n",
    "    if position:\n",
    "        classes_sentencas.insert(i, doc_no_processos[0:position.start()])\n",
    "\n",
    "        doc_no_classes_to = doc_no_processos[position.end():]\n",
    "    else:\n",
    "        conteudo_sentencas.insert(i, doc_no_processos)\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the assuntos\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r' - ', doc_no_classes_to)\n",
    "    if position:\n",
    "        assuntos_sentencas.insert(i, doc_no_classes_to[0:position.start()])\n",
    "\n",
    "        doc_no_assuntos_to = doc_no_classes_to[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the parts\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r' - ', doc_no_assuntos_to)\n",
    "    if position:\n",
    "        partes_sentencas.insert(i, doc_no_assuntos_to[0:position.start()])\n",
    "\n",
    "        doc_no_parts_to = doc_no_assuntos_to[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the content\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the ADV\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r\"(?i)- ADV\", doc_no_parts_to)\n",
    "    if position:\n",
    "        advs_sentencas.insert(i, doc_no_parts_to[position.start():])\n",
    "\n",
    "        conteudo_sentencas.insert(i, doc_no_parts_to[0: position.start()])\n",
    "    else:\n",
    "        conteudo_sentencas.insert(i, doc_no_parts_to)\n",
    "\n",
    "len(advs_sentencas), len(classes_sentencas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc_series = pd.Series(sp_data['Conteúdo'])\n",
    "\n",
    "ind_range = (data > 4)\n",
    "ind_range\n",
    "\n",
    "data_in_6_and_9 = sp_data['Conteúdo'][ind_range]\n",
    "\n",
    "positions_all = []\n",
    "\n",
    "processos_all = []\n",
    "\n",
    "classes_all = []\n",
    "\n",
    "assuntos_all = []\n",
    "\n",
    "partes_all = []\n",
    "\n",
    "conteudo_all = []\n",
    "\n",
    "advs_all = []\n",
    "\n",
    "for document in data_in_6_and_9:\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the processes\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    position = re.search(r' - ', document)\n",
    "    processos_all.append(document[0:position.start()])\n",
    "\n",
    "    doc_no_processos = document[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the classes\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r' - ', doc_no_processos)\n",
    "    classes_all.append(doc_no_processos[0:position.start()])\n",
    "\n",
    "    doc_no_classes_to = doc_no_processos[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the assuntos\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r' - ', doc_no_classes_to)\n",
    "    assuntos_all.append(doc_no_classes_to[0:position.start()])\n",
    "\n",
    "    doc_no_assuntos_to = doc_no_classes_to[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the parts\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r' - ', doc_no_assuntos_to)\n",
    "    partes_all.append(doc_no_assuntos_to[0:position.start()])\n",
    "\n",
    "    doc_no_parts_to = doc_no_assuntos_to[position.end():]\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the content\n",
    "    # ----------------------------------------------------------\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Filtering the ADV\n",
    "    # ----------------------------------------------------------\n",
    "    position = re.search(r\"(?i)- ADV\", doc_no_parts_to)\n",
    "    if position:\n",
    "        advs_all.append(doc_no_parts_to[position.start():])\n",
    "\n",
    "        conteudo_all.append(doc_no_parts_to[0: position.start()])\n",
    "    else:\n",
    "        conteudo_all.append(doc_no_parts_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
    "#     model = Sequential()\n",
    "#     model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "#     model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "#     model.add(layers.GlobalMaxPooling1D())\n",
    "#     model.add(layers.Dense(10, activation='relu'))\n",
    "#     model.add(layers.Dense(1, activation='sigmoid'))\n",
    "#     model.compile(optimizer='adam',\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# param_grid = dict(num_filters=[32, 64, 128],\n",
    "#                   kernel_size=[3, 5, 7],\n",
    "#                   vocab_size=[5000], \n",
    "#                   embedding_dim=[50],\n",
    "#                   maxlen=[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# # Main settings\n",
    "# epochs = 20\n",
    "# embedding_dim = 50\n",
    "# maxlen = 100\n",
    "# output_file = 'data/output.txt'\n",
    "\n",
    "# # Run grid search for each source (yelp, amazon, imdb)\n",
    "# for source, frame in df.groupby('source'):\n",
    "#     print('Running grid search for data set :', source)\n",
    "#     sentences = df['sentence'].values\n",
    "#     y = df['label'].values\n",
    "\n",
    "#     # Train-test split\n",
    "#     sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "#         sentences, y, test_size=0.25, random_state=1000)\n",
    "\n",
    "#     # Tokenize words\n",
    "#     tokenizer = Tokenizer(num_words=5000)\n",
    "#     tokenizer.fit_on_texts(sentences_train)\n",
    "#     X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "#     X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "#     # Adding 1 because of reserved 0 index\n",
    "#     vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "#     # Pad sequences with zeros\n",
    "#     X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "#     X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "#     # Parameter grid for grid search\n",
    "#     param_grid = dict(num_filters=[32, 64, 128],\n",
    "#                       kernel_size=[3, 5, 7],\n",
    "#                       vocab_size=[vocab_size],\n",
    "#                       embedding_dim=[embedding_dim],\n",
    "#                       maxlen=[maxlen])\n",
    "#     model = KerasClassifier(build_fn=create_model,\n",
    "#                             epochs=epochs, batch_size=10,\n",
    "#                             verbose=False)\n",
    "#     grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
    "#                               cv=4, verbose=1, n_iter=5)\n",
    "#     grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "#     # Evaluate testing set\n",
    "#     test_accuracy = grid.score(X_test, y_test)\n",
    "\n",
    "#     # Save and evaluate results\n",
    "#     prompt = input(f'finished {source}; write to file and proceed? [y/n]')\n",
    "#     if prompt.lower() not in {'y', 'true', 'yes'}:\n",
    "#         break\n",
    "#     with open(output_file, 'a') as f:\n",
    "#         s = ('Running {} data set\\nBest Accuracy : '\n",
    "#              '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n",
    "#         output_string = s.format(\n",
    "#             source,\n",
    "#             grid_result.best_score_,\n",
    "#             grid_result.best_params_,\n",
    "#             test_accuracy)\n",
    "#         print(output_string)\n",
    "#         f.write(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/rafael/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rafael/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rafael/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rafael/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rafael/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rafael/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rafael/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rafael/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rafael/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rafael/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rafael/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rafael/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# TOKENIZER THE \"CONTEUDO\" OF SENTENCES \n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer(num_words=5000)\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(conteudo_sentencas)\n",
    "\n",
    "# result = hashing_trick(conteudo_sentencas[15], round(len(\n",
    "#     words)*1.3), hash_function='md5', filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# palavras e numero de aparições nos documentos\n",
    "# t.word_counts\n",
    "\n",
    "# ordened = {k: v for k, v in sorted(t.word_docs.items(), key=lambda item: item[1], reverse = True)}\n",
    "# series = pd.Series(conteudo).str.contains('ação')\n",
    "# # melhores código e civil\n",
    "# series = pd.Series(conteudo)[series]\n",
    "# series\n",
    "# quantidade de documentos\n",
    "# t.document_count,\n",
    "# indice de cada palavra\n",
    "# len(t.word_index),\n",
    "# dicionario que mostra a palavra e em quantos documentos ela apareceu\n",
    "# t.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# GETTING THE TRAIN AND TEST DATASETS \n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "series = pd.Series(conteudo_sentencas)\n",
    "not_sentence = pd.Series(conteudo)\n",
    "\n",
    "X_train = pd.Series(conteudo_sentencas)\n",
    "not_sentence_train = not_sentence.sample(len(X_train), random_state=123)\n",
    "\n",
    "Y_train = [1] * len(X_train)\n",
    "Y_sentence_train = [0] * len(X_train)\n",
    "\n",
    "X_train = pd.concat([X_train, not_sentence_train])\n",
    "Y_train = Y_train + Y_sentence_train\n",
    "\n",
    "X_train = t.texts_to_matrix(X_train, mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 é sentenças e 0 é não sentenças\n",
    "X_train = X_train/np.max(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rafael/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/rafael/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 18368 samples, validate on 4592 samples\n",
      "Epoch 1/100\n",
      "18368/18368 [==============================] - 327s 18ms/step - loss: 0.6648 - accuracy: 0.6247 - val_loss: 0.9421 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "18368/18368 [==============================] - 317s 17ms/step - loss: 0.6630 - accuracy: 0.6250 - val_loss: 0.9914 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      " 2080/18368 [==>...........................] - ETA: 4:31 - loss: 0.6573 - accuracy: 0.6351"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-02ca5ed6d225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m           validation_split=0.2)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------------------\n",
    "# CREATING A SIMPLE MODEL \n",
    "# ------------------------------------------------------------------------------------------\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import Input, LSTM\n",
    "import tensorflow as tf\n",
    "\n",
    "# run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "\n",
    "# set parameters:\n",
    "batch_size = 16\n",
    "embedding_dims = 1024\n",
    "filters = 128\n",
    "kernel_size = 3\n",
    "hidden_dims = 256\n",
    "epochs = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(5000,\n",
    "                    embedding_dims,\n",
    "                    input_length=5000))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# model.add(LSTM(lstm_kernels, return_sequences=True))\n",
    "# model.add(LSTM(lstm_kernels, return_sequences=True))\n",
    "# model.add(LSTM(lstm_kernels, return_sequences=True))\n",
    "\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'],\n",
    "#               options = run_opts\n",
    "             )\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_1024Embbeddingd5000Word100EPOCHS.hd5')\n",
    "# model = keras.models.load_model(\n",
    "#     '/home/rafael/Documents/Projeto IC Direito/codes/pre-processing/model_1024Embbeddingd.hd5')\n",
    "tudo_10k = t.texts_to_matrix(conteudo[0:10000], mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(tudo_10k, batch_size=2)\n",
    "# predict = model.predict(X_test, batch_size = 16)\n",
    "\n",
    "sentencas = predict > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cont = pd.Series(conteudo[0:10000])\n",
    "# sentencas = sentencas[0:10000]\n",
    "# sent = np.array(sentencas.reshape(10000))\n",
    "# cont[sent]\n",
    "# np.unique(tudo_10k[0]),np.unique(X_train[0])\n",
    "# np.unique(sentencas, return_counts = True)\n",
    "# conteudo[0],tudo_10k[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordance(word, variable):\n",
    "\n",
    "    for i in range(2,10):\n",
    "\n",
    "        for sent in conteudo_token_sentencas:\n",
    "            if word in sent:\n",
    "                pos = sent.index(word)\n",
    "                left = \" \".join(sent[pos - i: pos])\n",
    "                right = \" \".join(sent[pos + 1: pos + i])\n",
    "                variable.append(f\"{left} {word} {right}\")\n",
    "\n",
    "\n",
    "palavras = ['processo',\n",
    "            'artigo',\n",
    "            'autos',\n",
    "            'código',\n",
    "            'termos',\n",
    "            'civil',\n",
    "            'custas',\n",
    "            'fls',\n",
    "            'honorários',\n",
    "            'ação']\n",
    "\n",
    "teste = []\n",
    "\n",
    "for word in palavras:\n",
    "\n",
    "    concordance(word, teste)\n",
    "\n",
    "l = []\n",
    "\n",
    "for i, t in enumerate(teste):\n",
    "\n",
    "    l.append([t, teste.count(teste[i])])\n",
    "dict(l)\n",
    "\n",
    "dictionary_all_words = {k: v for k, v in sorted(\n",
    "    dict(l).items(), key=lambda item: item[1], reverse=True)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
